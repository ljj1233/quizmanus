import pytest
from src.graph.crawler.crawler import Crawler

# è¿™é‡Œæ”¾å…¥ä½ æƒ³æµ‹è¯•çš„å›½å†… URL
# 1. CSDN åšå®¢æ–‡ç« ï¼ˆæŠ€æœ¯ç±»ï¼‰
# 2. çŸ¥ä¹ä¸“æ æ–‡ç« ï¼ˆç›¸å¯¹å¥½çˆ¬ï¼‰
# 3. ç™¾åº¦ç™¾ç§‘ï¼ˆæœ€ç¨³å®šçš„å›½å†…æµ‹è¯•æºï¼‰
TEST_URLS = [
    "https://zhuanlan.zhihu.com/p/1893871287308366230",                 # çŸ¥ä¹ Python ä¸“æ 
]

@pytest.mark.parametrize("url", TEST_URLS)
def test_crawler_returns_markdown_content(url: str):
    print(f"\nğŸš€ æ­£åœ¨æµ‹è¯•æŠ“å–: {url}")
    crawler = Crawler()
    
    try:
        article = crawler.crawl(url)
    except Exception as exc:
        # å¦‚æœç½‘ç»œä¸é€šï¼ˆæ¯”å¦‚æœåŠ¡å™¨åœ¨æµ·å¤–è¿ä¸ä¸Šå›½å†…ï¼Œæˆ–è€…å›½å†…è¿ä¸ä¸Šç‰¹å®šç«™ç‚¹ï¼‰ï¼Œè·³è¿‡
        pytest.skip(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {exc}")

    # æ–­è¨€ 1: åªè¦æ ‡é¢˜ä¸æ˜¯ "Error"ï¼Œå°±è¯´æ˜è¯·æ±‚é€šäº†
    # (Crawler ç±»é‡Œå¦‚æœ fetch å¤±è´¥ä¼šè¿”å› title="Error")
    assert article.title != "Error", f"âŒ çˆ¬è™«è¢«æ‹¦æˆªæˆ–å¤±è´¥! é”™è¯¯ä¿¡æ¯: {article.content}"
    
    # æ–­è¨€ 2: å†…å®¹ä¸èƒ½ä¸ºç©º
    assert article.content, "âŒ æŠ“å–åˆ°çš„å†…å®¹ä¸ºç©º"
    
    # æ–­è¨€ 3: å†…å®¹é•¿åº¦è¦è¶³å¤Ÿï¼ˆé¿å…åªæŠ“åˆ° '403 Forbidden' æˆ–éªŒè¯ç æç¤ºï¼‰
    # ä¸­æ–‡ç½‘é¡µé€šå¸¸åŒ…å«å¤§é‡å…ƒæ•°æ®ï¼ŒTrafilatura æå–åä¸€èˆ¬éƒ½ä¼šè¶…è¿‡ 100 å­—
    assert len(article.content) > 50, f"âŒ å†…å®¹å¤ªçŸ­ ({len(article.content)} chars)ï¼Œå¯èƒ½è¢«åçˆ¬æ‹¦æˆªäº†"

    # æ‰“å°ç»“æœçœ‹çœ‹
    print(f"âœ… æˆåŠŸ! æ ‡é¢˜: {article.title}")
    print(f"ğŸ“„ å†…å®¹é¢„è§ˆ: {article.content[:100].replace(chr(10), ' ')}...") # æ‰“å°å‰100ä¸ªå­—
    
    # æ–­è¨€ 4: URL åº”è¯¥ä¸€è‡´
    assert article.url == url